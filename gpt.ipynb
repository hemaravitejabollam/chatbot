!pip install huggingface_hub
!pip install transformers

from transformers import AutoTokenizer, AutoModelForCausalLM

# Load the tokenizer and model from Hugging Face
model_name = "EleutherAI/gpt-neo-1.3B"  # You can replace this with any Hugging Face model
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

def chat_with_model(user_input):
    # Tokenize the input text
    inputs = tokenizer(user_input, return_tensors="pt")

    # Generate a response from the model
    outputs = model.generate(
        inputs["input_ids"],
        max_length=100,  # Limit the length of the response
        num_return_sequences=1,  # Number of responses
        do_sample=True,  # Sampling for more creative responses
        top_k=50,  # Limits to top-k choices to reduce randomness
        temperature=0.7  # Controls the creativity (lower is less creative)
    )

    # Decode the model's output and return the response
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response

# Chat loop
print("Chatbot: Hello! How can I assist you today?")
while True:
    user_input = input("You: ")
    if user_input.lower() in ["exit", "quit", "bye"]:
        print("Chatbot: Goodbye! Have a great day!")
        break

    response = chat_with_model(user_input)
    print(f"Chatbot: {response}")
